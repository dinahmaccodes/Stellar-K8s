name: Performance Benchmark

on:
  push:
    branches: [main, develop]
    tags: ['v*']
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      baseline_version:
        description: 'Baseline version to compare against'
        required: false
        default: 'latest'
      regression_threshold:
        description: 'Regression threshold percentage'
        required: false
        default: '10'

env:
  CARGO_TERM_COLOR: always
  K6_VERSION: '0.49.0'
  PYTHON_VERSION: '3.11'

jobs:
  # ==========================================================================
  # Build Operator for Benchmarking
  # ==========================================================================
  build:
    name: Build Operator
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      image_tag: ${{ steps.version.outputs.image_tag }}
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Determine version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
          else
            VERSION="sha-$(git rev-parse --short HEAD)"
          fi
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "image_tag=ghcr.io/${{ github.repository }}:benchmark-$VERSION" >> $GITHUB_OUTPUT
      
      - name: Install Rust toolchain
        uses: dtolnay/rust-action@stable
      
      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
      
      - name: Build release binary
        run: cargo build --release
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          load: true
          tags: ${{ steps.version.outputs.image_tag }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ==========================================================================
  # Performance Benchmark Suite
  # ==========================================================================
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    needs: build
    
    services:
      # Use k3s for lightweight Kubernetes
      k3s:
        image: rancher/k3s:v1.28.4-k3s2
        options: --privileged
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg \
            --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | \
            sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6
      
      - name: Setup kind cluster
        uses: helm/kind-action@v1
        with:
          cluster_name: benchmark-cluster
          config: |
            kind: Cluster
            apiVersion: kind.x-k8s.io/v1alpha4
            nodes:
              - role: control-plane
              - role: worker
              - role: worker
      
      - name: Install CRDs and operator
        run: |
          kubectl apply -f stellarnode-crd.yaml
          kubectl create namespace stellar-system || true
          
          # Deploy operator for benchmarking
          kubectl apply -f - <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: stellar-operator
            namespace: stellar-system
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: stellar-operator
            template:
              metadata:
                labels:
                  app: stellar-operator
              spec:
                containers:
                  - name: operator
                    image: ${{ needs.build.outputs.image_tag }}
                    imagePullPolicy: IfNotPresent
                    ports:
                      - containerPort: 8080
                        name: http
                      - containerPort: 9090
                        name: metrics
                    resources:
                      requests:
                        cpu: 500m
                        memory: 512Mi
                      limits:
                        cpu: 2000m
                        memory: 2Gi
          EOF
          
          kubectl wait --for=condition=available deployment/stellar-operator -n stellar-system --timeout=120s
      
      - name: Setup port forwarding
        run: |
          kubectl port-forward -n stellar-system svc/stellar-operator 8080:8080 &
          kubectl proxy --port=8001 &
          sleep 5
      
      - name: Download baseline
        id: baseline
        run: |
          BASELINE_VERSION="${{ inputs.baseline_version || 'latest' }}"
          
          if [[ "$BASELINE_VERSION" == "latest" ]]; then
            # Find latest baseline
            BASELINE_FILE=$(ls -t benchmarks/baselines/*.json 2>/dev/null | head -1)
            if [[ -z "$BASELINE_FILE" ]]; then
              BASELINE_FILE="benchmarks/baselines/v0.1.0.json"
            fi
          else
            BASELINE_FILE="benchmarks/baselines/${BASELINE_VERSION}.json"
          fi
          
          echo "baseline_file=$BASELINE_FILE" >> $GITHUB_OUTPUT
          
          if [[ -f "$BASELINE_FILE" ]]; then
            echo "Using baseline: $BASELINE_FILE"
          else
            echo "No baseline found, will create one from this run"
          fi
      
      - name: Run k6 benchmarks
        id: k6
        run: |
          mkdir -p results
          
          k6 run \
            --env BASE_URL=http://localhost:8080 \
            --env K8S_API_URL=http://localhost:8001 \
            --env NAMESPACE=stellar-benchmark \
            --env RUN_ID=${{ github.run_id }} \
            --env VERSION=${{ needs.build.outputs.version }} \
            --env GIT_SHA=${{ github.sha }} \
            --env BASELINE_FILE=${{ steps.baseline.outputs.baseline_file }} \
            --out json=results/k6-output.json \
            benchmarks/k6/operator-load-test.js
        continue-on-error: true
      
      - name: Compare with baseline
        id: regression
        run: |
          THRESHOLD="${{ inputs.regression_threshold || '10' }}"
          
          python benchmarks/scripts/compare_benchmarks.py compare \
            --current results/benchmark-summary.json \
            --baseline ${{ steps.baseline.outputs.baseline_file }} \
            --threshold $THRESHOLD \
            --output results/regression-report.json \
            --fail-on-regression \
            --verbose
        continue-on-error: true
      
      - name: Collect operator logs
        if: always()
        run: |
          kubectl logs -n stellar-system -l app=stellar-operator --tail=500 > results/operator-logs.txt
          kubectl get events -n stellar-benchmark --sort-by='.lastTimestamp' > results/events.txt || true
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
          retention-days: 30
      
      - name: Create benchmark summary
        run: |
          echo "## üìä Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Version:** ${{ needs.build.outputs.version }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ -f results/benchmark-summary.json ]]; then
            echo "### Metrics" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat results/benchmark-summary.json | jq '.metrics' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            if [[ "$PASSED" == "true" ]]; then
              echo "### ‚úÖ Regression Check: PASSED" >> $GITHUB_STEP_SUMMARY
            else
              echo "### ‚ùå Regression Check: FAILED" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Regressions detected:**" >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
              cat results/regression-report.json | jq '.regressions' >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            fi
          fi
      
      - name: Check regression status
        if: steps.regression.outcome == 'failure'
        run: |
          echo "::error::Performance regression detected! See benchmark results for details."
          exit 1

  # ==========================================================================
  # Update Baseline (on release tags only)
  # ==========================================================================
  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/v')
    
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Create new baseline
        run: |
          VERSION=${{ needs.build.outputs.version }}
          
          python benchmarks/scripts/compare_benchmarks.py baseline \
            --input results/benchmark-summary.json \
            --output benchmarks/baselines/${VERSION}.json \
            --version ${VERSION}
      
      - name: Commit baseline
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          git add benchmarks/baselines/
          git commit -m "chore: update performance baseline for ${{ needs.build.outputs.version }}" || true
          git push

  # ==========================================================================
  # Performance Report (PR Comment)
  # ==========================================================================
  report:
    name: Post PR Report
    runs-on: ubuntu-latest
    needs: [build, benchmark]
    if: github.event_name == 'pull_request'
    permissions:
      pull-requests: write
    
    steps:
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ needs.build.outputs.version }}
          path: results/
      
      - name: Generate PR comment
        id: comment
        run: |
          COMMENT="## üìä Performance Benchmark Report\n\n"
          COMMENT+="**Version:** ${{ needs.build.outputs.version }}\n"
          COMMENT+="**Commit:** ${{ github.sha }}\n\n"
          
          if [[ -f results/benchmark-summary.json ]]; then
            TPS=$(cat results/benchmark-summary.json | jq -r '.metrics.tps.avg // "N/A"')
            P95=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p95 // "N/A"')
            P99=$(cat results/benchmark-summary.json | jq -r '.metrics.http_req_duration.p99 // "N/A"')
            ERRORS=$(cat results/benchmark-summary.json | jq -r '.metrics.error_rate // "N/A"')
            
            COMMENT+="### Key Metrics\n"
            COMMENT+="| Metric | Value |\n"
            COMMENT+="|--------|-------|\n"
            COMMENT+="| TPS | ${TPS} req/s |\n"
            COMMENT+="| Latency (p95) | ${P95} ms |\n"
            COMMENT+="| Latency (p99) | ${P99} ms |\n"
            COMMENT+="| Error Rate | ${ERRORS} |\n\n"
          fi
          
          if [[ -f results/regression-report.json ]]; then
            PASSED=$(cat results/regression-report.json | jq -r '.overall_passed')
            SUMMARY=$(cat results/regression-report.json | jq -r '.summary')
            
            if [[ "$PASSED" == "true" ]]; then
              COMMENT+="### ‚úÖ Regression Check: PASSED\n"
            else
              COMMENT+="### ‚ùå Regression Check: FAILED\n"
              COMMENT+="\n**Summary:** ${SUMMARY}\n\n"
              
              REGRESSIONS=$(cat results/regression-report.json | jq -r '.regressions | length')
              if [[ "$REGRESSIONS" -gt 0 ]]; then
                COMMENT+="<details>\n<summary>View Regressions</summary>\n\n"
                COMMENT+="\`\`\`json\n"
                COMMENT+="$(cat results/regression-report.json | jq '.regressions')\n"
                COMMENT+="\`\`\`\n</details>\n"
              fi
            fi
          fi
          
          COMMENT+="\n---\n*Benchmarks run on GitHub Actions*"
          
          # Save to file (GitHub Actions struggles with multiline outputs)
          echo -e "$COMMENT" > comment.md
      
      - name: Post comment
        uses: peter-evans/create-or-update-comment@v4
        with:
          issue-number: ${{ github.event.pull_request.number }}
          body-path: comment.md
